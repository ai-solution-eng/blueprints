# ------------------------------------------------------------
# The following values are for the link on the AIE UI.
# ------------------------------------------------------------
ezua:
  virtualService:
    endpoint: "ai-q.${DOMAIN_NAME}"
    istioGateway: "istio-system/ezaf-gateway"

# ------------------------------------------------------------
# The following values are for the AIQ AIRA backend service.
# ------------------------------------------------------------

replicaCount: 1

# The name of the image pull secret to use for the AIQ container images.
# Either create the secret manually and update the name here
# or update the imagePullSecret.password with your NGC API key
ngcImagePullSecretName: "ngc-secret"

# As we create it manually upfront this can be left on create: false
imagePullSecret:
  create: false
  name: "ngc-secret"
  registry: "nvcr.io"
  username: "$oauthtoken"
  password: "" 

# The image repository and tag for the AIQ AIRA backend service.
image:  
  repository: nvcr.io/nvidia/blueprint/aira-backend
  tag: v1.1.0
  pullPolicy: IfNotPresent

# The service type and port for the main AIQ AIRA backend service
service:
  port: 3838

# Update each value according to your desired configuration.
config:
 # REQUIRED to edit if a different namespace was used for NVIDIA Rag Blueprint deployment:
  rag_blueprint_namespace: nv-nvidia-blueprint-rag # Edit if you used a different namespace for NVIDIA Rag Blueprint deployment
  rag_ingest_url: "http://ingestor-server.nv-nvidia-blueprint-rag.svc.cluster.local:8082" # Edit if you used a different namespace for NVIDIA Rag Blueprint deployment, it's linking to a specific service in the form of http://ingestor-server.${NAMESPACE of RAG Blueprint Deployment}.svc.cluster.local
  rag_url: "http://rag-server.nv-nvidia-blueprint-rag.svc.cluster.local:8081" # Edit if you used a different namespace for NVIDIA Rag Blueprint deployment, it's linking to a specific service in the form of http://rag-server.${NAMESPACE of RAG Blueprint Deployment}.svc.cluster.local:${PORT})
  rag_api_key: "" # Typically not required
  milvus_host: "milvus.nv-nvidia-blueprint-rag.svc.cluster.local" # Edit if you used a different namespace for NVIDIA Rag Blueprint deployment, it's linking to a specific service in the form of http://milvus.${NAMESPACE of RAG Blueprint Deployment}.svc.cluster.local
  milvus_port: "19530"  # Typically doesn't change, edit if you used a different port in the NVIDIA RAG Blueprint deployment as preconfigured

# REQUIRED to fill out according to model choice and deployment beforehand:
  # The instruct_ settings are for the general purpose Q&A LLM
  instruct_model_name: "<REQUIRED: ADD LLM MODEL NAME e.g. meta/llama-3.1-8b-instruct>"
  instruct_temperature: "0.0"
  instruct_api_key: "<REQUIRED: ADD MLIS TOKEN or NGC API KEY if you want to use NVIDIA API>" 
  instruct_base_url: "<REQUIRED: ADD MLIS ENDPOINT/v1 or https://integrate.api.nvidia.com/v1 if you want to use NVIDIA API>"
  # The nemotron_ settings are for the reasoning LLM
  nemotron_api_key: "<REQUIRED: NGC API KEY>"  
  nemotron_model_name: "nvidia/llama-3.3-nemotron-super-49b-v1"
  nemotron_temperature: "0.5"
  nemotron_base_url: "https://integrate.api.nvidia.com/v1" # this uses the NVIDIA API, if you want to swap for a local model deploy it locally with MLIS and edit accordingly
  nemotron_max_tokens: "5000"
  nemotron_stream: "true"
  # Enter your Tavily API key here to enable web search
  tavily_api_key: "<OPTIONAL: IF YOU WANT TO USE WEBSEARCH CREATE A TAVILY ACCOUNT AND PASTE YOUR API KEY HERE>" #UPDATE THIS


# Do not update this command. It is the default command to launch the AI-Q backend service.
command: "/entrypoint.sh"


# ------------------------------------------------------------
# The following values are for the instruct LLM service
# The nemotron llm is assumed to be deployed via the RAG helm chart
# ------------------------------------------------------------

# As we create it manually upfront this can be left on create: false
ngcApiSecret:
  name: "ngc-api"
  password: "" 
  create: false

nim-llm:
  enabled: false
  service:
    name: "nim-llm"
  image:
      repository: nvcr.io/nim/meta/llama-3.3-70b-instruct
      pullPolicy: IfNotPresent
      tag: "1.8.5"
  resources:
    limits:
      nvidia.com/gpu: 2
    requests:
      nvidia.com/gpu: 2
  model:
    name: "meta/llama-3.3-70b-instruct"

# ------------------------------------------------------------
# The following values are for the AIQ AIRA frontend service.
# ------------------------------------------------------------

# The frontend application is a React web app. We recommend a NodePort so the frontend will be accessible at <your-node-ip>:3001
frontend:
  enabled: true
  service:
    port: 3001
    targetPort: 3001

  image:
    repository: nvcr.io/nvidia/blueprint/aira-frontend
    tag: v1.1.0
    pullPolicy: IfNotPresent

  replicaCount: 1

# ------------------------------------------------------------
# The following values are optional utility services
# ------------------------------------------------------------

# Enables the Phoenix tracing service
phoenix:
  enabled: true
  image:
    repository: arizephoenix/phoenix
    tag: latest
    pullPolicy: IfNotPresent
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 200m
      memory: 256Mi