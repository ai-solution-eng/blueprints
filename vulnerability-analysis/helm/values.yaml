# Default values for morpheus-vuln-analysis.
# This is a YAML-formatted file.
# Declare variables to be substituted into your templates.

# Namespace for deployment
#namespace: hpe-vuln-blueprint

# Application image configuration
image:
  # Set to lr1 for prod. Using personal dockerhub for testing.
  repository: srujanreddya/morpheus-vuln-analysis
  tag: 0.0.2-8f7a58c
  pullPolicy: IfNotPresent

# Nginx cache sidecar configuration
nginxCache:
  image: srujanreddya/nginx-cache
  tag: 0.0.2-8f7a58c
  imagePullPolicy: IfNotPresent
  ports:
    - containerPort: 80    # ✅ Correct NGINX port
  probes:
    liveness:
      path: "/"
      port: 80            # ✅ Correct NGINX port
      initialDelaySeconds: 10
      periodSeconds: 10
    readiness:
      path: "/"
      port: 80            # ✅ Correct NGINX port
      initialDelaySeconds: 5
      periodSeconds: 5
  volumeMounts:
    nginx-config: "/etc/nginx/nginx.conf"
    nginx-logs: "/var/log/nginx"
    service-cache: "/server_cache_intel"
    llm-cache: "/server_cache_llm"
  pvc:
    enabled: true
    size: 10Gi # Size of the persistent volume for nginx cache. Is this size sufficient?
    #storageClass: "nfs-csi"
  pvcName: "vuln-shared-pvc"

# Service configuration
service:
  type: ClusterIP
  ports:
    http: 80              # ✅ Correct NGINX port
    agentApi: 8000        # ✅ Jupyter notebook port
    scanApi: 26466        # ✅ Morpheus API port

# Volume configuration
volumes:
  nginxConfig:
    enabled: true
    configMapName: configmap
  
  nginxLogs:
    enabled: true
    type: emptyDir
    sizeLimit: "1Gi"
  
  serviceCache:
    enabled: true
    type: persistentVolumeClaim
    claimName: vuln-shared-pvc
    sizeLimit: "10Gi"
  
  llmCache:
    enabled: true
    type: persistentVolumeClaim
    claimName: vuln-shared-pvc
    sizeLimit: "10Gi"
  
  workspace:
    enabled: false
    type: persistentVolumeClaim
    claimName: vuln-shared-pvc
    sizeLimit: "5Gi"

# Secret configuration for both Morpheus and nginx-cache containers
secrets:
  name: morpheus-nginx-secrets
  NVD_API_KEY: <YOUR_NVD_API_KEY>
  GHSA_API_KEY: <YOUR_GHSA_API_KEY>
  NGC_API_KEY: ""
  NGC_ORG_ID: ""
  NVIDIA_API_KEY: <YOUR_NVIDIA_API_KEY>
  OPENAI_API_KEY: ""
  SERPAPI_API_KEY: <YOUR_SERPAPI_API_KEY>
  NGINX_UPSTREAM_NVAI: "https://api.nvcf.nvidia.com"
  NGINX_UPSTREAM_NIM_LLM: "https://integrate.api.nvidia.com"
  NGINX_UPSTREAM_NIM_EMBED: "https://integrate.api.nvidia.com"

# Resource limits and requests
resources:
  requests:
    memory: "8Gi"
    cpu: "500m"
    nvidia.com/gpu: 1
  limits:
    memory: "16Gi"
    cpu: "1000m"
    nvidia.com/gpu: 1

nginxCacheResources:
  requests:
    cpu: "100m"
  limits:
    memory: "16Gi"
    cpu: "1000m"
    
# Environment variables for the main application
env:
  TERM: "xterm"
  HF_HUB_CACHE: "/tmp/huggingface_cache"
  XDG_CACHE_HOME: "/tmp/cache"
  
  # API Base URLs

  # Base URLs for various vulnerability and threat intelligence APIs
  # All can be OPTIONAL, but will limit access to security data
  # Common Weakness Enumeration API base URL
  CWE_DETAILS_BASE_URL: "http://localhost:80/cwe-details"
  # Deps.dev API base URL
  DEPSDEV_BASE_URL: "http://localhost:80/depsdev"
  # FIRST.org API base URL
  FIRST_BASE_URL: "http://localhost:80/first"
  # GitHub Security Advisory API base URL
  GHSA_BASE_URL: "http://localhost:80/ghsa"
  # Red Hat Security Advisory API base URL
  RHSA_BASE_URL: "http://localhost:80/rhsa"
  # SerpAPI base URL 
  SERPAPI_BASE_URL: "http://localhost:80/serpapi"
  # Ubuntu Security Notices API base URL
  UBUNTU_BASE_URL: "http://localhost:80/ubuntu"
  # National Vulnerability Database API base URL 
  NVD_BASE_URL: "http://localhost:80/nvd"

  # URLs for accessing AI services. Require either NVIDIA_API_BASE or OPENAI_API_BASE & OPENAI_BASE_URL.
  # NVIDIA NGC API (OPTIONAL: Only needed for using NVIDIA hosted NIMs)
  NGC_API_BASE: "http://localhost:80/nemo/v1"
  # NIM Embedder API base URL (MANDATORY)
  NIM_EMBED_BASE_URL: "http://localhost:80/nim_embed/v1"
  # NVIDIA API base URL (OPTIONAL: needed if using NVIDIA NIMs)
  NVIDIA_API_BASE: "http://localhost:80/nim_llm/v1"
  # OpenAI API base URL (OPTIONAL: needed if using OpenAI LLMs)
  OPENAI_API_BASE: ""
  # OpenAI base URL (OPTIONAL: needed if using OpenAI LLMs)
  OPENAI_BASE_URL: ""

  # Model names for LLM/embedding tasks. All are MANDATORY if overriding the default value of '-meta/llama-3.1-70b-instruct'
  # Checklist generation LLM model name 
  CHECKLIST_MODEL_NAME: meta/llama-3.1-70b-instruct
  # Code VDB retriever queries vector DB built from source code 
  CODE_VDB_RETRIEVER_MODEL_NAME: meta/llama-3.1-70b-instruct
  # Doc VDB retriever queries vector DB built from docs 
  DOC_VDB_RETRIEVER_MODEL_NAME: meta/llama-3.1-70b-instruct
  # CVE agent executor model name. Executes task list provided by checklist generation node.
  CVE_AGENT_EXECUTOR_MODEL_NAME: meta/llama-3.1-70b-instruct
  # Summarization model name. Summarizes results of agent executor.
  SUMMARIZE_MODEL_NAME: meta/llama-3.1-70b-instruct
  # Justification model name. Provides final CVE justification and rating. 
  JUSTIFY_MODEL_NAME: meta/llama-3.1-70b-instruct
  # Embedding model name. Creates embeddings from source code and docs.
  EMBEDDER_MODEL_NAME: nvidia/nv-embedqa-e5-v5

  #Settings for the task agent
  # Controls whether task agent prints details for individual steps taken. Enable for debugging.
  RETURN_INTERMEDIATE_STEPS:  true
  # Controls max number of intermediate steps taken by task agent. 
  MAX_ITERATIONS: 10
  # Controls the maximum number of concurrent requests to the LLM. 
  # 5 is used to avoid 429 Error - Too many requests to LLM. Lower if still encountering 429.
  MAX_CONCURRENCY: 5 
# Container configuration
containers:
  morpheus:
    name: morpheus-vuln-analysis
    ports:
      - containerPort: 26466  # ✅ Main Morpheus API
      - containerPort: 8000   # ✅ Jupyter notebook
    envFrom:
      - secretRef:
          name: morpheus-nginx-secrets
  
  nginx:
    name: nginx-cache
    image: ${NGINX_IMAGE_REPOSITORY}
    tag: ${NGINX_TAG}
    ports:
      - containerPort: 80     # ✅ NGINX port
    envFrom:
      - secretRef:
          name: morpheus-nginx-secrets

# Probes configuration
probes:
  liveness:
    enabled: true
    path: /evaluate/jobs
    port: 26466
    initialDelaySeconds: 30
    periodSeconds: 180
    timeoutSeconds: 5
    failureThreshold: 3
  
  readiness:
    enabled: true
    path: /evaluate/jobs
    port: 26466
    initialDelaySeconds: 30
    periodSeconds: 180
    timeoutSeconds: 5
    failureThreshold: 3

# Pod security context
securityContext:
  runAsNonRoot: true
  runAsUser: 1000
  runAsGroup: 1000
  fsGroup: 1000

# Container security context
containerSecurityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  capabilities:
    drop:
      - ALL

# Node selector for pod placement
nodeSelector: {}

# Tolerations for pod scheduling
tolerations: []

# Affinity for pod scheduling
affinity: {}

# Pod annotations
podAnnotations: {}

# Deployment annotations
deploymentAnnotations: {}

# Service annotations
serviceAnnotations: {}

# Number of replicas
replicaCount: 1

# Deployment strategy and pod template
deployment:
  annotations: {}
  podAnnotations: {}
  restartPolicy: Always
  selectorLabels:
    app: "{{ include \"morpheus-vuln-analysis.name\" . }}"

# Update strategy
updateStrategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 1
    maxSurge: 1

# Persistent volumes configuration
persistence:
  enabled: false
  storageClass: ""
  accessMode: ReadWriteOnce
  size: 8Gi

# Ingress configuration
ingress:
  enabled: false
  className: ""
  annotations: {}
  hosts:
    - host: morpheus-vuln-analysis.local
      paths:
        - path: /
          pathType: Prefix
  tls: []

# External NIM Services Configuration
externalServices:
  # Self-hosted NIM LLM service
  nimLLM:
    enabled: true
    # Replace this URL with the appropriate endpoint for your environment.
    baseUrl: https://integrate.api.nvidia.com
    model: "meta/llama-3.1-70b-instruct"
  
  # Self-hosted NIM Embedding service
  nimEmbedding:
    enabled: true
    # Replace this URL with the appropriate endpoint for your environment.
    baseUrl: https://integrate.api.nvidia.com
    model: nvidia/nv-embedqa-e5-v5

# Extra environment variables
extraEnv: []

#Shared PVC for the nginx cache & ezwrapper
vulnSharedPvc:  vuln-shared-pvc

ezua:
  virtualService:
    endpoint: "morpheus-vulnerability-analysis.${DOMAIN_NAME}"
    istioGateway: "istio-system/ezaf-gateway"