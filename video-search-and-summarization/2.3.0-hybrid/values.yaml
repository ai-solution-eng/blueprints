
# Copyright (c) 2023, NVIDIA CORPORATION.  All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.  Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.

ezua:
  virtualService:
    endpoint: "vss.${DOMAIN_NAME}"
    istioGateway: "istio-system/ezaf-gateway"
  # if your cluster has self-signed certs, and you're using an MLIS endpoint, update this to true
  # and then uncomment out the extrapod volume and args section below
  selfSignedCert: true

global: {ngcImagePullSecretName: "ngc-docker-reg-secret"}
nvcf:
  dockerRegSecrets: []
  additionalSecrets: []
  localStorageProvisioner: []
vss:
  applicationSpecs:
    vss-deployment:
      containers:
        vss:
          env:
          - name: FRONTEND_PORT
            value: '9000'
          - name: BACKEND_PORT
            value: '8000'
          - name: GRAPH_DB_URI
            value: bolt://neo-4-j-service:7687
          - name: GRAPH_DB_USERNAME
            value: neo4j
          - name: GRAPH_DB_PASSWORD
            value: password
          - name: MILVUS_DB_HOST
            value: milvus-milvus-deployment-milvus-service
          - name: MILVUS_DB_PORT
            value: '19530'
          - name: VLM_MODEL_TO_USE
            value: nvila
          - name: MODEL_PATH
            value: ngc:nvidia/tao/nvila-highres:nvila-lite-15b-highres-lita
          - name: DISABLE_GUARDRAILS
            value: 'false'
          - name: OPENAI_API_KEY_NAME
            value: VSS_OPENAI_API_KEY
          - name: NVIDIA_API_KEY_NAME
            value: VSS_NVIDIA_API_KEY
          - name: NGC_API_KEY_NAME
            value: VSS_NGC_API_KEY
          - name: TRT_LLM_MODE
            value: int4_awq
          - name: VLM_BATCH_SIZE
            value: ''
          - name: VIA_VLM_OPENAI_MODEL_DEPLOYMENT_NAME
            value: ''
          - name: VIA_VLM_ENDPOINT
            value: ''
          - name: VIA_VLM_API_KEY
            value: ''
          - name: OPENAI_API_VERSION
            value: ''
          - name: AZURE_OPENAI_API_VERSION
            value: ''
          - name: ENABLE_AUDIO
            value: 'true'
          - name: RIVA_ASR_SERVER_URI
            value: riva-service
          - name: RIVA_ASR_GRPC_PORT
            value: '50051'
          - name: RIVA_ASR_HTTP_PORT
            value: '9000'
          - name: ENABLE_RIVA_SERVER_READINESS_CHECK
            value: 'false'
          - name: RIVA_ASR_SERVER_IS_NIM
            value: 'true'
          - name: RIVA_ASR_SERVER_USE_SSL
            value: 'false'
          - name: RIVA_ASR_SERVER_API_KEY
            value: ''
          - name: RIVA_ASR_SERVER_FUNC_ID
            value: ''
          - name: INSTALL_PROPRIETARY_CODECS
            value: 'true'
          - name: NVIDIA_API_KEY
            valueFrom:
              secretKeyRef:
                name: ngc-api-key-secret
                key: NGC_API_KEY
          resources:
            requests:
              cpu: "2"
              memory: "8Gi"
              nvidia.com/gpu: 1
            limits:
              cpu: "8"
              memory: "32Gi"
              nvidia.com/gpu: 1
      initContainers:
      - command:
        - sh
        - -c
        - until nc -z -w 2 milvus-milvus-deployment-milvus-service 19530; do echo
          waiting for milvus; sleep 2; done
        image: busybox:1.28
        imagePullPolicy: IfNotPresent
        name: check-milvus-up
      - command:
        - sh
        - -c
        - until nc -z -w 2 neo-4-j-service 7687; do echo waiting for neo4j; sleep
          2; done
        image: busybox:1.28
        imagePullPolicy: IfNotPresent
        name: check-neo4j-up
      # - args:
      #   - "while ! curl -s -f -o /dev/null http://llm-nim-svc:8000/v1/health/live;\
      #     \ do\n  echo \"Waiting for LLM...\"\n  sleep 2\ndone\n"
        # command:
        # - sh
        # - -c
        # image: curlimages/curl:latest
        # name: check-llm-up
  llmModel: meta/llama-3.1-70b-instruct
  llmModelChat: meta/llama-3.1-70b-instruct
  vlmModelPath: ngc:nim/nvidia/vila-1.5-40b:vila-yi-34b-siglip-stage3_1003_video_v8
  vlmModelType: vila-1.5
  configs:
    bridge.mp4.graph_rag.yaml:
      graph_rag:
        embedding:
          base_url: http://nemo-embedding-embedding-deployment-embedding-service:8000/v1
        reranker:
          base_url: "https://integrate.api.nvidia.com/v1"
    ca_rag_config.yaml:
      chat:
        embedding:
          base_url: "http://nemo-embedding-embedding-deployment-embedding-service:8000/v1"
        llm:
          base_url: "https://integrate.api.nvidia.com/v1"
          model: meta/llama-3.1-70b-instruct
        reranker:
          base_url: "https://integrate.api.nvidia.com/v1"
      notification:
        llm:
          base_url: "https://integrate.api.nvidia.com/v1"
          model: meta/llama-3.1-70b-instruct
      summarization:
        embedding:
          base_url: http://nemo-embedding-embedding-deployment-embedding-service:8000/v1
        llm:
          base_url: "https://integrate.api.nvidia.com/v1"
          model: meta/llama-3.1-70b-instruct
    graph_rag_config.yaml:
      graph_rag:
        embedding:
          base_url: http://nemo-embedding-embedding-deployment-embedding-service:8000/v1
        reranker:
          base_url: "https://integrate.api.nvidia.com/v1"
    guardrails_config.yaml:
      models:
      - engine: nim
        model: meta/llama-3.1-70b-instruct
        parameters:
          base_url: "https://integrate.api.nvidia.com/v1"
        type: main
      - engine: nim
        model: nvidia/llama-3.2-nv-embedqa-1b-v2
        parameters:
          base_url: http://nemo-embedding-embedding-deployment-embedding-service:8000/v1
        type: embeddings
    its.mp4.graph_rag.yaml:
      graph_rag:
        embedding:
          base_url: http://nemo-embedding-embedding-deployment-embedding-service:8000/v1
        reranker:
          base_url: "https://integrate.api.nvidia.com/v1"
    warehouse.mp4.graph_rag.yaml:
      graph_rag:
        embedding:
          base_url: http://nemo-embedding-embedding-deployment-embedding-service:8000/v1
        reranker:
          base_url: "https://integrate.api.nvidia.com/v1"
    warehouse_82min.mp4.graph_rag.yaml:
      graph_rag:
        embedding:
          base_url: http://nemo-embedding-embedding-deployment-embedding-service:8000/v1
        reranker:
          base_url: "https://integrate.api.nvidia.com/v1"
  extraPodVolumes:
  - name: secret-ngc-api-key-volume
    secret:
      secretName: ngc-api-key-secret
      items:
      - key: NGC_API_KEY
        path: ngc-api-key
  - name: secret-graph-db-username-volume
    secret:
      secretName: graph-db-creds-secret
      items:
      - key: username
        path: graph-db-username
  - name: secret-graph-db-password-volume
    secret:
      secretName: graph-db-creds-secret
      items:
      - key: password
        path: graph-db-password
  extraPodVolumeMounts:
  - name: secret-ngc-api-key-volume
    mountPath: /secrets/ngc-api-key
    subPath: ngc-api-key
    readOnly: true
  - name: secret-graph-db-username-volume
    mountPath: /secrets/graph-db-username
    subPath: graph-db-username
    readOnly: true
  - name: secret-graph-db-password-volume
    mountPath: /secrets/graph-db-password
    subPath: graph-db-password
    readOnly: true
  egress:
    milvus:
      address: milvus-milvus-deployment-milvus-service
      port: 19530
    neo4j-bolt:
      address: neo-4-j-service
      port: 7687
    llm-openai-api:
      address: llm-nim-svc
      port: 8000
    nemo-embed:
      address: nemo-embedding-embedding-deployment-embedding-service
      port: 8000
    nemo-rerank:
      address: nemo-rerank-ranking-deployment-ranking-service
      port: 8000
    riva-api:
      address: riva-service
      port: 9000
milvus:
  resources:
    requests:
      cpu: "500m"
      memory: "1Gi"
    limits:
      cpu: "2"
      memory: "2Gi"
      nvidia.com/gpu: 0
  applicationSpecs:
    milvus-deployment:
      containers:
        milvus-container:
          env:
          - name: ETCD_ENDPOINTS
            value: etcd-etcd-deployment-etcd-service:2379
          - name: MINIO_ADDRESS
            value: minio-minio-deployment-minio-service:9010
          - name: KNOWHERE_GPU_MEM_POOL_SIZE
            value: 2048;4096
  image:
    all:
      repository: milvusdb/milvus
      tag: v2.3.7
      pullPolicy: IfNotPresent
    tools:
      repository: milvusdb/milvus-config-tool
      tag: v0.1.2
      pullPolicy: IfNotPresent
  ingress:
    annotations:
      nginx.ingress.kubernetes.io/backend-protocol: GRPC
      nginx.ingress.kubernetes.io/listen-ports-ssl: '[19530]'
      nginx.ingress.kubernetes.io/proxy-body-size: 4m
      nginx.ingress.kubernetes.io/ssl-redirect: 'true'
    labels: {}
    rules:
    - host: milvus-example.local
      path: /
      pathType: Prefix
    tls: []
  metrics:
    enabled: true
    serviceMonitor:
      enabled: false
      interval: 30s
      scrapeTimeout: 10s
      additionalLabels: {}
  fullnameOverride: milvus
  cluster:
    enabled: false
  labels: {}
  annotations: {}
  extraConfigFiles:
    user.yaml: "#    For example enable rest http for milvus proxy\n#    proxy:\n\
      #      http:\n#        enabled: true\n##  Enable tlsMode and set the tls cert\
      \ and key\n#  tls:\n#    serverPemPath: /etc/milvus/certs/tls.crt\n#    serverKeyPath:\
      \ /etc/milvus/certs/tls.key\n#   common:\n#     security:\n#       tlsMode:\
      \ 1\n\n"
  service:
    type: ClusterIP
    port: 19530
    portName: milvus
    nodePort: ''
    annotations: {}
    labels: {}
    externalIPs: []
    loadBalancerSourceRanges:
    - 0.0.0.0/0
  serviceAccount:
    create: false
    name: null
    annotations: null
    labels: null
  livenessProbe:
    enabled: true
    initialDelaySeconds: 90
    periodSeconds: 30
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 5
  readinessProbe:
    enabled: true
    initialDelaySeconds: 90
    periodSeconds: 10
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 5
  log:
    level: info
    file:
      maxSize: 300
      maxAge: 10
      maxBackups: 20
    format: text
    persistence:
      mountPath: /milvus/logs
      enabled: false
      annotations:
        helm.sh/resource-policy: keep
      persistentVolumeClaim:
        existingClaim: ''
        storageClass: null
        accessModes: ReadWriteMany
        size: 10Gi
        subPath: ''
  heaptrack:
    image:
      repository: milvusdb/heaptrack
      tag: v0.1.0
      pullPolicy: IfNotPresent
  standalone:
    replicas: 1
    resources:
      limits:
        nvidia.com/gpu: null
    nodeSelector: {}
    affinity: {}
    tolerations: []
    extraEnv: []
    heaptrack:
      enabled: false
    disk:
      enabled: true
      size:
        enabled: false
    profiling:
      enabled: false
    messageQueue: rocksmq
    persistence:
      mountPath: /var/lib/milvus
      enabled: true
      annotations:
        helm.sh/resource-policy: keep
      persistentVolumeClaim:
        existingClaim: ''
        storageClass: null
        accessModes: ReadWriteOnce
        size: 50Gi
        subPath: ''
  proxy:
    enabled: true
    replicas: 1
    resources: {}
    nodeSelector: {}
    affinity: {}
    tolerations: []
    extraEnv: []
    heaptrack:
      enabled: false
    profiling:
      enabled: false
    http:
      enabled: true
      debugMode:
        enabled: false
    tls:
      enabled: false
  rootCoordinator:
    enabled: true
    replicas: 1
    resources: {}
    nodeSelector: {}
    affinity: {}
    tolerations: []
    extraEnv: []
    heaptrack:
      enabled: false
    profiling:
      enabled: false
    activeStandby:
      enabled: false
    service:
      port: 53100
      annotations: {}
      labels: {}
      clusterIP: ''
  queryCoordinator:
    enabled: true
    replicas: 1
    resources: {}
    nodeSelector: {}
    affinity: {}
    tolerations: []
    extraEnv: []
    heaptrack:
      enabled: false
    profiling:
      enabled: false
    activeStandby:
      enabled: false
    service:
      port: 19531
      annotations: {}
      labels: {}
      clusterIP: ''
  queryNode:
    enabled: true
    replicas: 1
    resources: {}
    nodeSelector: {}
    affinity: {}
    tolerations: []
    extraEnv: []
    heaptrack:
      enabled: false
    disk:
      enabled: true
      size:
        enabled: false
    profiling:
      enabled: false
  indexCoordinator:
    enabled: true
    replicas: 1
    resources: {}
    nodeSelector: {}
    affinity: {}
    tolerations: []
    extraEnv: []
    heaptrack:
      enabled: false
    profiling:
      enabled: false
    activeStandby:
      enabled: false
    service:
      port: 31000
      annotations: {}
      labels: {}
      clusterIP: ''
  indexNode:
    enabled: true
    replicas: 1
    resources: {}
    nodeSelector: {}
    affinity: {}
    tolerations: []
    extraEnv: []
    heaptrack:
      enabled: false
    profiling:
      enabled: false
    disk:
      enabled: true
      size:
        enabled: false
  dataCoordinator:
    enabled: true
    replicas: 1
    resources: {}
    nodeSelector: {}
    affinity: {}
    tolerations: []
    extraEnv: []
    heaptrack:
      enabled: false
    profiling:
      enabled: false
    activeStandby:
      enabled: false
    service:
      port: 13333
      annotations: {}
      labels: {}
      clusterIP: ''
  dataNode:
    enabled: true
    replicas: 1
    resources: {}
    nodeSelector: {}
    affinity: {}
    tolerations: []
    extraEnv: []
    heaptrack:
      enabled: false
    profiling:
      enabled: false
  mixCoordinator:
    enabled: false
    replicas: 1
    resources: {}
    nodeSelector: {}
    affinity: {}
    tolerations: []
    extraEnv: []
    heaptrack:
      enabled: false
    profiling:
      enabled: false
    activeStandby:
      enabled: false
    service:
      annotations: {}
      labels: {}
      clusterIP: ''
  attu:
    enabled: false
    name: attu
    image:
      repository: zilliz/attu
      tag: v2.2.8
      pullPolicy: IfNotPresent
    service:
      annotations: {}
      labels: {}
      type: ClusterIP
      port: 3000
    resources: {}
    podLabels: {}
    ingress:
      enabled: false
      annotations: {}
      labels: {}
      hosts:
      - milvus-attu.local
      tls: []
  minio:
    enabled: true
    name: minio
    mode: null
    image:
      tag: RELEASE.2023-03-20T20-16-18Z
      pullPolicy: IfNotPresent
    accessKey: minioadmin
    secretKey: minioadmin
    existingSecret: ''
    bucketName: milvus-bucket
    rootPath: file
    useIAM: false
    iamEndpoint: ''
    region: ''
    useVirtualHost: false
    podDisruptionBudget:
      enabled: false
    resources:
      requests:
        memory: 2Gi
    gcsgateway:
      enabled: false
      replicas: 1
      gcsKeyJson: /etc/credentials/gcs_key.json
      projectId: ''
    service:
      type: ClusterIP
      port: 9000
    persistence:
      enabled: true
      existingClaim: ''
      storageClass: null
      accessMode: ReadWriteOnce
      size: 500Gi
    livenessProbe:
      enabled: true
      initialDelaySeconds: 5
      periodSeconds: 5
      timeoutSeconds: 5
      successThreshold: 1
      failureThreshold: 5
    readinessProbe:
      enabled: true
      initialDelaySeconds: 5
      periodSeconds: 5
      timeoutSeconds: 1
      successThreshold: 1
      failureThreshold: 5
    startupProbe:
      enabled: true
      initialDelaySeconds: 0
      periodSeconds: 10
      timeoutSeconds: 5
      successThreshold: 1
      failureThreshold: 60
  etcd:
    enabled: true
    name: etcd
    replicaCount: null
    pdb:
      create: false
    image:
      repository: milvusdb/etcd
      tag: 3.5.5-r4
      pullPolicy: IfNotPresent
    service:
      type: ClusterIP
      port: 2379
      peerPort: 2380
    auth:
      rbac:
        enabled: false
    persistence:
      enabled: true
      storageClass: null
      accessMode: ReadWriteOnce
      size: 10Gi
    livenessProbe:
      enabled: true
      timeoutSeconds: 10
    readinessProbe:
      enabled: true
      periodSeconds: 20
      timeoutSeconds: 10
    autoCompactionMode: revision
    autoCompactionRetention: '1000'
    extraEnvVars:
    - name: ETCD_QUOTA_BACKEND_BYTES
      value: '4294967296'
    - name: ETCD_HEARTBEAT_INTERVAL
      value: '500'
    - name: ETCD_ELECTION_TIMEOUT
      value: '2500'
  pulsar:
    enabled: false
    name: pulsar
    fullnameOverride: ''
    persistence: true
    maxMessageSize: '5242880'
    rbac:
      enabled: false
      psp: false
      limit_to_namespace: true
    affinity:
      anti_affinity: false
    components:
      zookeeper: true
      bookkeeper: true
      autorecovery: true
      broker: true
      functions: false
      proxy: true
      toolset: false
      pulsar_manager: false
    monitoring:
      prometheus: false
      grafana: false
      node_exporter: false
      alert_manager: false
    images:
      broker:
        repository: apachepulsar/pulsar
        pullPolicy: IfNotPresent
        tag: 2.8.2
      autorecovery:
        repository: apachepulsar/pulsar
        tag: 2.8.2
        pullPolicy: IfNotPresent
      zookeeper:
        repository: apachepulsar/pulsar
        pullPolicy: IfNotPresent
        tag: 2.8.2
      bookie:
        repository: apachepulsar/pulsar
        pullPolicy: IfNotPresent
        tag: 2.8.2
      proxy:
        repository: apachepulsar/pulsar
        pullPolicy: IfNotPresent
        tag: 2.8.2
      pulsar_manager:
        repository: apachepulsar/pulsar-manager
        pullPolicy: IfNotPresent
        tag: v0.1.0
    zookeeper:
      resources:
        requests:
          memory: 1024Mi
          cpu: 0.3
      configData:
        PULSAR_MEM: "-Xms1024m -Xmx1024m\n"
        PULSAR_GC: "-Dcom.sun.management.jmxremote -Djute.maxbuffer=10485760 -XX:+ParallelRefProcEnabled\
          \ -XX:+UnlockExperimentalVMOptions -XX:+DoEscapeAnalysis -XX:+DisableExplicitGC\
          \ -XX:+PerfDisableSharedMem -Dzookeeper.forceSync=no\n"
      pdb:
        usePolicy: false
    bookkeeper:
      replicaCount: 3
      volumes:
        journal:
          name: journal
          size: 100Gi
        ledgers:
          name: ledgers
          size: 200Gi
      resources:
        requests:
          memory: 2048Mi
          cpu: 1
      configData:
        PULSAR_MEM: "-Xms4096m -Xmx4096m -XX:MaxDirectMemorySize=8192m\n"
        PULSAR_GC: "-Dio.netty.leakDetectionLevel=disabled -Dio.netty.recycler.linkCapacity=1024\
          \ -XX:+UseG1GC -XX:MaxGCPauseMillis=10 -XX:+ParallelRefProcEnabled -XX:+UnlockExperimentalVMOptions\
          \ -XX:+DoEscapeAnalysis -XX:ParallelGCThreads=32 -XX:ConcGCThreads=32 -XX:G1NewSizePercent=50\
          \ -XX:+DisableExplicitGC -XX:-ResizePLAB -XX:+ExitOnOutOfMemoryError -XX:+PerfDisableSharedMem\
          \ -XX:+PrintGCDetails\n"
        nettyMaxFrameSizeBytes: '104867840'
      pdb:
        usePolicy: false
    broker:
      component: broker
      podMonitor:
        enabled: false
      replicaCount: 1
      resources:
        requests:
          memory: 4096Mi
          cpu: 1.5
      configData:
        PULSAR_MEM: "-Xms4096m -Xmx4096m -XX:MaxDirectMemorySize=8192m\n"
        PULSAR_GC: "-Dio.netty.leakDetectionLevel=disabled -Dio.netty.recycler.linkCapacity=1024\
          \ -XX:+ParallelRefProcEnabled -XX:+UnlockExperimentalVMOptions -XX:+DoEscapeAnalysis\
          \ -XX:ParallelGCThreads=32 -XX:ConcGCThreads=32 -XX:G1NewSizePercent=50\
          \ -XX:+DisableExplicitGC -XX:-ResizePLAB -XX:+ExitOnOutOfMemoryError\n"
        maxMessageSize: '104857600'
        defaultRetentionTimeInMinutes: '10080'
        defaultRetentionSizeInMB: '-1'
        backlogQuotaDefaultLimitGB: '8'
        ttlDurationDefaultInSeconds: '259200'
        subscriptionExpirationTimeMinutes: '3'
        backlogQuotaDefaultRetentionPolicy: producer_exception
      pdb:
        usePolicy: false
    autorecovery:
      resources:
        requests:
          memory: 512Mi
          cpu: 1
    proxy:
      replicaCount: 1
      podMonitor:
        enabled: false
      resources:
        requests:
          memory: 2048Mi
          cpu: 1
      service:
        type: ClusterIP
      ports:
        pulsar: 6650
      configData:
        PULSAR_MEM: "-Xms2048m -Xmx2048m\n"
        PULSAR_GC: "-XX:MaxDirectMemorySize=2048m\n"
        httpNumThreads: '100'
      pdb:
        usePolicy: false
    pulsar_manager:
      service:
        type: ClusterIP
    pulsar_metadata:
      component: pulsar-init
      image:
        repository: apachepulsar/pulsar
        tag: 2.8.2
  kafka:
    enabled: false
    name: kafka
    replicaCount: 3
    image:
      repository: bitnami/kafka
      tag: 3.1.0-debian-10-r52
    terminationGracePeriodSeconds: '90'
    pdb:
      create: false
    startupProbe:
      enabled: true
    heapOpts: -Xmx4096m -Xms4096m
    maxMessageBytes: _10485760
    defaultReplicationFactor: 3
    offsetsTopicReplicationFactor: 3
    logRetentionHours: 168
    logRetentionBytes: _-1
    extraEnvVars:
    - name: KAFKA_CFG_MAX_PARTITION_FETCH_BYTES
      value: '5242880'
    - name: KAFKA_CFG_MAX_REQUEST_SIZE
      value: '5242880'
    - name: KAFKA_CFG_REPLICA_FETCH_MAX_BYTES
      value: '10485760'
    - name: KAFKA_CFG_FETCH_MESSAGE_MAX_BYTES
      value: '5242880'
    - name: KAFKA_CFG_LOG_ROLL_HOURS
      value: '24'
    persistence:
      enabled: true
      storageClass: null
      accessMode: ReadWriteOnce
      size: 300Gi
    metrics:
      kafka:
        enabled: false
        image:
          repository: bitnami/kafka-exporter
          tag: 1.4.2-debian-10-r182
      jmx:
        enabled: false
        image:
          repository: bitnami/jmx-exporter
          tag: 0.16.1-debian-10-r245
      serviceMonitor:
        enabled: false
    service:
      type: ClusterIP
      ports:
        client: 9092
    zookeeper:
      enabled: true
      replicaCount: 3
  externalS3:
    enabled: false
    host: ''
    port: ''
    accessKey: ''
    secretKey: ''
    useSSL: false
    bucketName: ''
    rootPath: ''
    useIAM: false
    cloudProvider: aws
    iamEndpoint: ''
    region: ''
    useVirtualHost: false
  externalGcs:
    bucketName: ''
  externalEtcd:
    enabled: false
    endpoints:
    - localhost:2379
  externalPulsar:
    enabled: false
    host: localhost
    port: 6650
    maxMessageSize: '5242880'
    tenant: public
    namespace: default
    authPlugin: ''
    authParams: ''
  externalKafka:
    enabled: false
    brokerList: localhost:9092
    securityProtocol: SASL_SSL
    sasl:
      mechanisms: PLAIN
      username: ''
      password: ''
  egress:
    etcd:
      address: etcd-etcd-deployment-etcd-service
      port: 2379
    minio:
      address: minio-minio-deployment-minio-service
      port: 9010
neo4j:
  resources:
    requests:
      cpu: "500m"
      memory: "1Gi"
    limits:
      cpu: "2"
      memory: "2Gi"
  extraPodVolumes:
  - name: secret-db-username-volume
    secret:
      secretName: graph-db-creds-secret
      items:
      - key: username
        path: db-username
  - name: secret-db-password-volume
    secret:
      secretName: graph-db-creds-secret
      items:
      - key: password
        path: db-password
  extraPodVolumeMounts:
  - name: secret-db-username-volume
    mountPath: /secrets/db-username
    subPath: db-username
    readOnly: true
  - name: secret-db-password-volume
    mountPath: /secrets/db-password
    subPath: db-password
    readOnly: true
# nim-llm:
#   env:
#   - name: NIM_MAX_MODEL_LEN
#     value: '128000'
#   image:
#     repository: nvcr.io/nim/meta/llama-3.1-70b-instruct
#     tag: 1.3.3
#   resources:
#     limits:
#       nvidia.com/gpu: 4
#   model:
#     name: meta/llama-3.1-70b-instruct
#     ngcAPISecret: ngc-api-key-secret
#   persistence:
#     enabled: true
#   hostPath:
#     enabled: true
#   service:
#     name: llm-nim-svc
#   llmModel: meta/llama-3.1-70b-instruct

nemo-embedding:
  resources:
    requests:
      cpu: "1"
      memory: "4Gi"
    limits:
      cpu: "4"
      memory: "16Gi"
      nvidia.com/gpu: 1

# nemo-rerank:
#   resources:
#     limits:
#       nvidia.com/gpu: 1

riva:
  enabled: true
  resources:
    requests:
      cpu: "1"
      memory: "4Gi"
    limits:
      cpu: "4"
      memory: "16Gi"
      nvidia.com/gpu: 1

etcd:
  resources: 
    requests:
      cpu: "500m"
      memory: "1Gi"
    limits:
      cpu: "2"
      memory: "2Gi"
minio:
  resources:
    requests:
      cpu: "500m"
      memory: "1Gi"
    limits:
      cpu: "2"
      memory: "2Gi"
